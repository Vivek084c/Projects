{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "#prevent oom\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "\n",
    "# Normalize images to the range [0, 1]\n",
    "x_train = (x_train / 255.0).astype(np.float32)\n",
    "x_test = (x_test / 255.0).astype(np.float32)\n",
    "\n",
    "# Flatten the images to be vectors\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype(np.float32)\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype(np.float32)\n",
    "\n",
    "# Convert the digit labels to even/odd labels:\n",
    "# Even -> 0, Odd -> 1\n",
    "y_train_even_odd = np.array([label % 2 for label in y_train], dtype=np.int32)\n",
    "y_test_even_odd = np.array([label % 2 for label in y_test], dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "device = \"/GPU:0\" if tf.config.list_physical_devices('GPU') else \"/CPU:0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayerTF:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        with tf.device(device):  # to run the operation on gpu\n",
    "            self.weights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.Gweights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.bias = tf.Variable(tf.zeros([1, num_neurons], dtype=tf.float32))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = tf.matmul(inputs, self.weights) + tf.matmul(inputs, self.Gweights) + self.bias\n",
    "\n",
    "    def update_weights(self, gradients, learning_rate=0.01):\n",
    "        self.weights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.Gweights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.bias.assign_sub(learning_rate * gradients[1])      # Update bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the activation functions\n",
    "\n",
    "class ActivationSigmoidTF:\n",
    "    def forward(self, inputs):\n",
    "        self.output = tf.nn.sigmoid(inputs)\n",
    "\n",
    "class LossBinaryCrossentropyTF:\n",
    "    def calculate(self, output, y_true):\n",
    "        output = tf.clip_by_value(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "        return tf.reduce_mean(- (y_true * tf.math.log(output) + (1 - y_true) * tf.math.log(1 - output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function \n",
    "@tf.function  # Compiles function for efficiency (Graph Mode)\n",
    "def train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass of the custom dense network\n",
    "        dense_layer.forward(X_batch)\n",
    "        # incorporating the activation function \n",
    "        activation.forward(dense_layer.output)\n",
    "        #calculating the loss values\n",
    "        loss = loss_function.calculate(activation.output, y_batch)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [dense_layer.weights, dense_layer.bias])\n",
    "\n",
    "    # Ensure valid gradients\n",
    "    if gradients is None or any(g is None for g in gradients):\n",
    "        return None  # If gradient calculation fails, return None\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [dense_layer.weights, dense_layer.bias]))\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = tf.cast(activation.output > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), dtype=tf.float32))\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the main training loop:\n",
    "def train_custom_nn(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01):\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    # Convert input data & labels to TensorFlow tensors\n",
    "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tf = tf.convert_to_tensor(y_train.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "    # Create TensorFlow dataset for efficient training\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
    "    dataset = dataset.shuffle(num_samples).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Initialize custom model\n",
    "    with tf.device(device):\n",
    "        dense_layer = CustomDenseLayerTF(X_train.shape[1], 1)\n",
    "        activation = ActivationSigmoidTF()\n",
    "        loss_function = LossBinaryCrossentropyTF()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Adam optimizer\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        total_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        num_batches = tf.Variable(0, dtype=tf.int32)\n",
    "\n",
    "        # Process data in batches using TensorFlow dataset\n",
    "        for X_batch, y_batch in dataset:\n",
    "            result = train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer)\n",
    "            \n",
    "            if result is None:\n",
    "                continue  # Skip this batch if train_step() returned None\n",
    "            \n",
    "            loss, accuracy = result\n",
    "            total_loss.assign_add(loss)\n",
    "            total_accuracy.assign_add(accuracy)\n",
    "            num_batches.assign_add(1)\n",
    "\n",
    "        # Compute average loss and accuracy per epoch\n",
    "        avg_loss = total_loss / tf.cast(num_batches, tf.float32)\n",
    "        avg_accuracy = total_accuracy / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss.numpy():.4f}, Accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "    \n",
    "    return dense_layer, activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:42:12.663250: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.3102, Accuracy: 0.8658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:42:13.129584: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - Loss: 0.2617, Accuracy: 0.8948\n",
      "Epoch 3/5 - Loss: 0.2512, Accuracy: 0.8993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-27 23:42:14.034830: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - Loss: 0.2446, Accuracy: 0.9038\n",
      "Epoch 5/5 - Loss: 0.2456, Accuracy: 0.9027\n"
     ]
    }
   ],
   "source": [
    "# Train the custom neural network with a batch size of 64\n",
    "dense_layer, activation = train_custom_nn(x_train, y_train_even_odd, epochs=5, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing with prebuild keras model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m785\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> (3.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m785\u001b[0m (3.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">785</span> (3.07 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m785\u001b[0m (3.07 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6971 - loss: 0.5676\n",
      "Epoch 2/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8570 - loss: 0.3602\n",
      "Epoch 3/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8635 - loss: 0.3261\n",
      "Epoch 4/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8725 - loss: 0.3050\n",
      "Epoch 5/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8814 - loss: 0.2855\n",
      "Epoch 6/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8869 - loss: 0.2782\n",
      "Epoch 7/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8861 - loss: 0.2734\n",
      "Epoch 8/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8915 - loss: 0.2648\n",
      "Epoch 9/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8996 - loss: 0.2626\n",
      "Epoch 10/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8953 - loss: 0.2620\n",
      "Epoch 11/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8981 - loss: 0.2511\n",
      "Epoch 12/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9014 - loss: 0.2443\n",
      "Epoch 13/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9003 - loss: 0.2472\n",
      "Epoch 14/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9035 - loss: 0.2505\n",
      "Epoch 15/15\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9043 - loss: 0.2472\n",
      "\n",
      "===== Comparison =====\n",
      "Custom Neural Network Accuracy: 0.8981\n",
      "TensorFlow Neural Network Accuracy: 0.9015\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "# ================== TENSORFLOW MODEL (Using GPU) ==================\n",
    "\n",
    "# Define the equivalent TensorFlow model\n",
    "with tf.device(device):  # Run on GPU\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(1, activation='sigmoid', input_shape=(28 * 28,))\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    # Train for 5 epochs\n",
    "    model.fit(x_train, y_train_even_odd, epochs=15, verbose=1, batch_size=64)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    loss_tf, accuracy_tf = model.evaluate(x_train, y_train_even_odd, verbose=0)\n",
    "\n",
    "print(\"\\n===== Comparison =====\")\n",
    "print(f\"Custom Neural Network Accuracy: {acc:.4f}\")\n",
    "print(f\"TensorFlow Neural Network Accuracy: {accuracy_tf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDenseLayerTF:\n",
    "    def __init__(self, num_inputs, num_neurons):\n",
    "        with tf.device(device):  # to run the operation on gpu\n",
    "            self.weights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.Gweights = tf.Variable(tf.random.normal([num_inputs, num_neurons], stddev=0.01, dtype=tf.float32))\n",
    "            self.bias = tf.Variable(tf.zeros([1, num_neurons], dtype=tf.float32))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = tf.matmul(inputs, self.weights) + tf.matmul(inputs, self.Gweights) + self.bias\n",
    "\n",
    "    def update_weights(self, gradients, learning_rate=0.01):\n",
    "        self.weights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.Gweights.assign_sub(learning_rate * gradients[0])  # Update weights\n",
    "        self.bias.assign_sub(learning_rate * gradients[1])      # Update bias\n",
    "\n",
    "    #defining the activation functions\n",
    "\n",
    "class ActivationSigmoidTF:\n",
    "    def forward(self, inputs):\n",
    "        self.output = tf.nn.sigmoid(inputs)\n",
    "\n",
    "class LossBinaryCrossentropyTF:\n",
    "    def calculate(self, output, y_true):\n",
    "        output = tf.clip_by_value(output, 1e-7, 1 - 1e-7)  # Avoid log(0)\n",
    "        return tf.reduce_mean(- (y_true * tf.math.log(output) + (1 - y_true) * tf.math.log(1 - output)))\n",
    "# training function \n",
    "@tf.function  # Compiles function for efficiency (Graph Mode)\n",
    "def train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass of the custom dense network\n",
    "        dense_layer.forward(X_batch)\n",
    "        # incorporating the activation function \n",
    "        activation.forward(dense_layer.output)\n",
    "        #calculating the loss values\n",
    "        loss = loss_function.calculate(activation.output, y_batch)\n",
    "\n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [dense_layer.weights, dense_layer.bias])\n",
    "\n",
    "    # Ensure valid gradients\n",
    "    if gradients is None or any(g is None for g in gradients):\n",
    "        return None  # If gradient calculation fails, return None\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.apply_gradients(zip(gradients, [dense_layer.weights, dense_layer.bias]))\n",
    "\n",
    "    # Compute accuracy\n",
    "    predictions = tf.cast(activation.output > 0.5, dtype=tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, y_batch), dtype=tf.float32))\n",
    "\n",
    "    return loss, accuracy\n",
    "#defining the main training loop:\n",
    "def train_custom_nn(X_train, y_train, epochs=5, batch_size=32, learning_rate=0.01):\n",
    "    num_samples = X_train.shape[0]\n",
    "\n",
    "    # Convert input data & labels to TensorFlow tensors\n",
    "    X_train_tf = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "    y_train_tf = tf.convert_to_tensor(y_train.reshape(-1, 1), dtype=tf.float32)\n",
    "\n",
    "    # Create TensorFlow dataset for efficient training\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_train_tf, y_train_tf))\n",
    "    dataset = dataset.shuffle(num_samples).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    # Initialize custom model\n",
    "    with tf.device(device):\n",
    "        dense_layer = CustomDenseLayerTF(X_train.shape[1], 1)\n",
    "        activation = ActivationSigmoidTF()\n",
    "        loss_function = LossBinaryCrossentropyTF()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)  # Adam optimizer\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "        total_accuracy = tf.Variable(0.0, dtype=tf.float32)\n",
    "        num_batches = tf.Variable(0, dtype=tf.int32)\n",
    "\n",
    "        # Process data in batches using TensorFlow dataset\n",
    "        for X_batch, y_batch in dataset:\n",
    "            result = train_step(dense_layer, activation, loss_function, X_batch, y_batch, optimizer)\n",
    "            \n",
    "            if result is None:\n",
    "                continue  # Skip this batch if train_step() returned None\n",
    "            \n",
    "            loss, accuracy = result\n",
    "            total_loss.assign_add(loss)\n",
    "            total_accuracy.assign_add(accuracy)\n",
    "            num_batches.assign_add(1)\n",
    "\n",
    "        # Compute average loss and accuracy per epoch\n",
    "        avg_loss = total_loss / tf.cast(num_batches, tf.float32)\n",
    "        avg_accuracy = total_accuracy / tf.cast(num_batches, tf.float32)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss.numpy():.4f}, Accuracy: {avg_accuracy.numpy():.4f}\")\n",
    "    \n",
    "    return dense_layer, activation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
